---
title: "Actuarial Applications of Deep Learning"
subtitle: "Loss Reserving and Beyond"
author: "Joe Fang, Nicole Foster, and Kevin Kuo"
date: "May 2018"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "hygge"]
    lib_dir: static/libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

# Welcome/Introduction

---
# Agenda

- Introduction to Deep Learning

- Hands-on Keras Demo

- Loss Reserving Case Study

- Q+A and Open Discussion

---
# What is Machine Learning?

- A field of computer science that gives computers the ability to "learn" with data, without being explicitly programmed

![](img/machine-learning-features.png)

---
# Why Deep Learning?

- Subset of machine learning

- Often uses a neural network to simulate how the human brain learns

- Performs better than traditional machine learning techniques for large datasets

```{r fig.align='center', out.width='60%', echo=FALSE}
knitr::include_graphics("img/deep-learning.png")
```

---
# What is a Neural Network?

![](img/neural-network.png)

---
# How does a neural network learn?

- Loss Functions

- Gradient Descent

![](img/gradient-descent.png)

---
# What software is available for deep learning?

- Front-end/programming language

    - R, Python

- Interface between front and back end

    - Keras

- Back-end for calculations

    - TensorFlow, Theano

---
# Intro to Keras

- Keras is a high-level neural networks API developed with a focus on enabling fast experimentation

- https://keras.rstudio.com/

---
# MNIST Example

Let's switch to R!

---
# Q&A and Discussion

---
class: inverse, center, middle

# Loss reserving case study

---

# "Claims liabilities Estimation"

Basically, figure out what we gotta pay in the future due to claims.

---

# Example triangle

```{r}
library(insuranceData)
data(IndustryAuto)
head(IndustryAuto, 12)
```

---

# Actually a triangle

```{r, message = FALSE}
library(tidyverse)
# look at data as of calendar year 2002
data <- IndustryAuto %>%
  rename(ay = Incurral.Year, dev = Development.Year, paid = Claim) %>%
  filter(ay + dev - 1 <= 2002)
# convert to triangle format
data %>%
  spread(dev, paid)
```

---

# Treat this as a predictive modeling problem

Each cell of the triangle is a row in the modeling dataset.

We just need to come up with some predictors

```{r, echo = FALSE}
data %>%
  mutate(predictors = "?!?!") %>%
  head(5)
```

Then we can do something like

```{r, eval = FALSE}
crazy_AI_algorithm(paid ~ predictors, data = data)
```

---

# Introducing DeepTriangle

Let's try to apply neural networks on some real reserving data. We're gonna call it **DeepTriangle* because it sounds cool.

---

# Data

Schedule P data from [http://www.casact.org/research/index.cfm?fa=loss_reserves_data](http://www.casact.org/research/index.cfm?fa=loss_reserves_data).

10 accident years (1988-1997) of paid and incurred losses, with 10 development lags, from a bunch of companies and lines of business.

---

# Response and predictors

Let's talk about our response variable and predictors!

---

# Response

- **Response: incremental paid losses and total claims outstanding**

We're gonna predict both paid loss and claims o/s in the same model, ain't that cool?!

---

# Predictors

- Response: incremental paid losses and total claims outstanding `r emo::ji("thumbsup")`
- **Predictors:**

---

# Predictors

Note that... there's really not much we can use in aggregated data. We also have to follow this rule:

> The information used to derive the predictors for a cell must be available before the calendar period associated with the cell.

I.e. we're not cheating and looking into the future.

---
# Predictors

- Response: incremental paid losses and total claims outstanding `r emo::ji("thumbsup")`
- **Predictors:**
  - **Time series of paid losses and case reserves along accident year**
  - **Time series of paid losses and case reserves along development year**

---

Let's see what we mean by "time series of paid losses".

---

# Predictors

Get some incremental numbers

```{r}
incremental_data <- data %>%
  group_by(ay) %>%
  mutate(incremental_paid = paid - lag(paid, default = 0))
incremental_data %>%
  select(-paid) %>%
  spread(dev, incremental_paid)
```

---

# Predictors

Let's say, for example, we want to compute "time series of paid losses along AY" predictor, we'd do something like

```{r}
sample_data_with_paid_history <- incremental_data %>%
  group_by(ay) %>%
  mutate(
    paid_history = map_chr(
      dev, # for each development year, get paid 
      # loss numbers up to the year before
      ~ incremental_paid[0:(.x - 1)] %>%
        paste0(collapse = ", ") # for printing
    )
  ) %>%
  ungroup() %>%
  filter(ay == 1995) %>%
  select(dev, incremental_paid, paid_history)
```

---

# Predictors

That looks like

```{r, out.width="100%"}
sample_data_with_paid_history
```

---

# Predictors

In DeepTriangle, we do this for both paid loss & case reserve numbers, along both `ay` and `dev`.

---

# Predictors

- Response: incremental paid losses and total claims outstanding `r emo::ji("thumbsup")`
- **Predictors:**
  - Time series of paid losses and case reserves along accident year `r emo::ji("thumbsup")`
  - Time series of paid losses and case reserves along development year `r emo::ji("thumbsup")`
  - **Company (because we're using data from all companies simultaneously)**

---

# Predictors

Company code is one-hot encoded, e.g. the third company in a collection of $20$ companies would be represented as

```{r}
keras::to_categorical(3, num_classes = 20)
```

Super easy!

---

# Predictors

- Response: incremental paid losses and total claims outstanding `r emo::ji("thumbsup")`
- Predictors:
  - Time series of paid losses and case reserves along accident year `r emo::ji("thumbsup")`
  - Time series of paid losses and case reserves along development year `r emo::ji("thumbsup")`
  - Company (because we're using data from all companies simultaneously) `r emo::ji("thumbsup")`

Now that we've gone through the response and predictors, let's talk about the neural network itself!

---

# Architecture

Looks fancy, but it's just a neural network!

```{r, out.width= "40%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/nn1.png")
```

---

# Embedding layer

Dimensionality reduction!

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/embedding_paper.png")
```

For example, company code `1767` might get mapped to `c(0.4, 1.2, -3.7)`.

---

# Neural network for sequences

Just like a vanilla feedforward neural network, except we feed the sequential input... in sequence.

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/rnn.png")
```

---

# Helping RNN remember

Gated recurrent unit (GRU) is an architecture that helps the network remember stuff from a long time ago. (Don't worry about the details!)

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/gru.png")
```

---

# Putting it all together

Again, we're really just applying a bunch of functions, one after another, to our input data.

```{r, out.width= "40%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/nn1.png")
```

---

# Some results

Sample results from the company with the most data in the dataset...

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/ppauto-results.png")
```

---

# Some results

Workers' comp

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/wkcomp-results.png")
```

---

# Benchmarking

Results for other methods taken from [http://www.casact.org/pubs/monographs/index.cfm?fa=meyers-monograph01](http://www.casact.org/pubs/monographs/index.cfm?fa=meyers-monograph01).

```{r, out.width= "70%", fig.align='center', echo = FALSE}
knitr::include_graphics("figs/comparison-results.png")
```

---

# Conclusion

(insert corgi: wow, AI, such hype)

---

# Future work

- It'd be cool to use on claims level data, where we can take into account things like claims adjusters' notes or photos
- Reserve variability
